{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison Notebook\n",
    "\n",
    "This notebook provides comprehensive analysis and comparison of trained models in the SentiCompare framework.\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. Load your benchmark results\n",
    "2. Explore model performance across metrics\n",
    "3. Compare models using statistical tests\n",
    "4. Generate insights and recommendations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Benchmark Results\n",
    "\n",
    "Load the benchmark results from your experiments directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your results\n",
    "RESULTS_PATH = \"experiments/results\"  # Adjust this path\n",
    "\n",
    "# Load results (you can also load from individual JSON files)\n",
    "try:\n",
    "    # Try to load aggregated results\n",
    "    results_df = pd.read_csv(f\"{RESULTS_PATH}/aggregated_results.csv\")\n",
    "    print(f\"Loaded {len(results_df)} results from aggregated CSV\")\n",
    "except FileNotFoundError:\n",
    "    # Fallback: look for individual result files\n",
    "    import json\n",
    "    import glob\n",
    "    \n",
    "    result_files = glob.glob(f\"{RESULTS_PATH}/*.json\")\n",
    "    all_results = []\n",
    "    \n",
    "    for file_path in result_files:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            if isinstance(data, dict):\n",
    "                all_results.append(data)\n",
    "            elif isinstance(data, list):\n",
    "                all_results.extend(data)\n",
    "    \n",
    "    if all_results:\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        print(f\"Loaded {len(results_df)} results from {len(result_files)} JSON files\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No results found in {RESULTS_PATH}\")\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nDataset shape: {results_df.shape}\")\n",
    "print(f\"\\nColumns: {list(results_df.columns)}\")\n",
    "print(f\"\\nModels: {results_df['model_name'].unique() if 'model_name' in results_df.columns else 'Unknown'}\")\n",
    "print(f\"\\nDatasets: {results_df['dataset'].unique() if 'dataset' in results_df.columns else 'Unknown'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performance Overview\n",
    "\n",
    "Let's examine the overall performance metrics across all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metric columns\n",
    "metric_cols = [col for col in results_df.columns if col.startswith('metric_')]\n",
    "print(f\"\\nFound metrics: {[col.replace('metric_', '') for col in metric_cols]}\")\n",
    "\n",
    "# Calculate summary statistics by model\n",
    "if 'model_name' in results_df.columns and metric_cols:\n",
    "    model_summary = results_df.groupby('model_name')[metric_cols].agg(['mean', 'std', 'min', 'max', 'count'])\n",
    "    \n",
    "    # Round for better display\n",
    "    model_summary = model_summary.round(4)\n",
    "    \n",
    "    print(\"\\nModel Performance Summary:\")\n",
    "    print(model_summary.to_string())\n",
    "else:\n",
    "    print(\"No model_name or metric columns found in results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizations\n",
    "\n",
    "Create various visualizations to compare model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison heatmap\n",
    "if len(metric_cols) >= 2 and 'model_name' in results_df.columns:\n",
    "    # Create pivot table for heatmap\n",
    "    pivot_data = results_df.pivot_table(\n",
    "        values=metric_cols[0],  # Use first metric for values\n",
    "        index='model_name',\n",
    "        columns='dataset',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(pivot_data, annot=True, cmap='RdYlBu_r', center=pivot_data.values.mean())\n",
    "    plt.title(f'{metric_cols[0].replace(\"metric_\", \"\").title()} Performance Heatmap')\n",
    "    plt.xlabel('Dataset')\n",
    "    plt.ylabel('Model')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Insufficient data for heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy vs Latency scatter plot\n",
    "if 'metric_accuracy' in results_df.columns and 'latency_mean_ms' in results_df.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create scatter with model names\n",
    "    for i, model in enumerate(results_df['model_name'].unique()):\n",
    "        model_data = results_df[results_df['model_name'] == model]\n",
    "        if not model_data.empty:\n",
    "            plt.scatter(\n",
    "                model_data['latency_mean_ms'], \n",
    "                model_data['metric_accuracy'], \n",
    "                label=model,\n",
    "                alpha=0.7,\n",
    "                s=60\n",
    "            )\n",
    "    \n",
    "    plt.xlabel('Latency (ms)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Accuracy vs Latency')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Accuracy or latency data not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-metric radar chart\n",
    "if len(metric_cols) >= 3 and 'model_name' in results_df.columns:\n",
    "    # Select top 3 metrics for radar chart\n",
    "    radar_metrics = metric_cols[:3]\n",
    "    \n",
    "    # Calculate average performance per model\n",
    "    model_avg = results_df.groupby('model_name')[radar_metrics].mean()\n",
    "    \n",
    "    # Prepare data for radar chart\n",
    "    models = model_avg.index.tolist()\n",
    "    values = model_avg.values.tolist()\n",
    "    metrics_labels = [m.replace('metric_', '').title() for m in radar_metrics]\n",
    "    \n",
    "    # Create radar chart\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    # Plot each model\n",
    "    angles = np.linspace(0, 2 * np.pi, len(metrics_labels), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    for i, (model, model_values) in enumerate(zip(models, values)):\n",
    "        values += model_values[:1]  # Complete the circle\n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=model)\n",
    "        ax.fill(angles, values, alpha=0.25)\n",
    "    \n",
    "    # Add labels\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metrics_labels)\n",
    "    ax.set_ylim(0, 1)\n",
    "    plt.title('Multi-Metric Performance Comparison')\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Insufficient metrics for radar chart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Analysis\n",
    "\n",
    "Perform statistical tests to determine significant differences between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison between top models\n",
    "if 'model_name' in results_df.columns and metric_cols:\n",
    "    # Get top 2 models by first metric\n",
    "    top_models = results_df.groupby('model_name')[metric_cols[0]].mean().nlargest(2).index.tolist()\n",
    "    \n",
    "    if len(top_models) >= 2:\n",
    "        model1, model2 = top_models[:2]\n",
    "        \n",
    "        print(f\"\\nComparing {model1} vs {model2}:\")\n",
    "        \n",
    "        # Compare across all datasets\n",
    "        for metric in metric_cols[:3]:  # Compare top 3 metrics\n",
    "            metric_col = f'metric_{metric}'\n",
    "            if metric_col in results_df.columns:\n",
    "                # Get paired data points\n",
    "                model1_data = []\n",
    "                model2_data = []\n",
    "                \n",
    "                for dataset in results_df['dataset'].unique():\n",
    "                    m1_data = results_df[(results_df['model_name'] == model1) & (results_df['dataset'] == dataset)][metric_col]\n",
    "                    m2_data = results_df[(results_df['model_name'] == model2) & (results_df['dataset'] == dataset)][metric_col]\n",
    "                    \n",
    "                    if not m1_data.empty and not m2_data.empty:\n",
    "                        model1_data.append(m1_data.iloc[0])\n",
    "                        model2_data.append(m2_data.iloc[0])\n",
    "                \n",
    "                if len(model1_data) >= 2 and len(model2_data) >= 2:\n",
    "                    # Perform paired t-test\n",
    "                    from scipy import stats\n",
    "                    \n",
    "                    t_stat, p_value = stats.ttest_rel(model1_data, model2_data)\n",
    "                    \n",
    "                    print(f\"  {metric}: t-statistic = {t_stat:.4f}, p-value = {p_value:.6f}\")\n",
    "                    print(f\"  Significant difference (α=0.05): {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "                    \n",
    "                    # Calculate effect size (Cohen's d)\n",
    "                    diff = np.array(model1_data) - np.array(model2_data)\n",
    "                    cohens_d = np.mean(diff) / np.std(diff, ddof=1)\n",
    "                    print(f\"  Effect size (Cohen's d): {cohens_d:.4f}\")\n",
    "            else:\n",
    "                print(f\"  Insufficient data for {metric} comparison\")\n",
    "    else:\n",
    "        print(\"Need at least 2 models for comparison\")\n",
    "else:\n",
    "    print(\"No model or metric data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Ranking\n",
    "\n",
    "Create comprehensive rankings based on multiple criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composite scoring (performance + speed)\n",
    "if 'model_name' in results_df.columns and metric_cols:\n",
    "    # Calculate performance score (average of first 2 metrics)\n",
    "    performance_metrics = metric_cols[:2]\n",
    "    \n",
    "    # Calculate speed score (inverse latency + throughput)\n",
    "    speed_metrics = [col for col in results_df.columns if any(x in col.lower() for x in ['latency', 'throughput'])]\n",
    "    \n",
    "    model_scores = []\n",
    "    \n",
    "    for model in results_df['model_name'].unique():\n",
    "        model_data = results_df[results_df['model_name'] == model]\n",
    "        \n",
    "        # Performance score (0-1 normalized)\n",
    "        perf_score = 0\n",
    "        for metric in performance_metrics:\n",
    "            if metric in model_data.columns:\n",
    "                values = model_data[metric].dropna()\n",
    "                if len(values) > 0:\n",
    "                    # Normalize to 0-1 scale (higher is better)\n",
    "                    min_val = values.min()\n",
    "                    max_val = values.max()\n",
    "                    if max_val > min_val:\n",
    "                        normalized = (values.mean() - min_val) / (max_val - min_val)\n",
    "                        perf_score += normalized / len(performance_metrics)\n",
    "        \n",
    "        # Speed score\n",
    "        speed_score = 0\n",
    "        speed_count = 0\n",
    "        \n",
    "        for metric in speed_metrics:\n",
    "            if metric in model_data.columns:\n",
    "                values = model_data[metric].dropna()\n",
    "                if len(values) > 0:\n",
    "                    speed_count += 1\n",
    "                    \n",
    "                    if 'latency' in metric.lower():\n",
    "                        # Lower latency is better (inverse scoring)\n",
    "                        min_val = values.min()\n",
    "                        max_val = values.max()\n",
    "                        if max_val > min_val:\n",
    "                            normalized = 1 - (values.mean() - min_val) / (max_val - min_val)\n",
    "                            speed_score += normalized\n",
    "                    elif 'throughput' in metric.lower():\n",
    "                        # Higher throughput is better\n",
    "                        min_val = values.min()\n",
    "                        max_val = values.max()\n",
    "                        if max_val > min_val:\n",
    "                            normalized = (values.mean() - min_val) / (max_val - min_val)\n",
    "                            speed_score += normalized\n",
    "        \n",
    "        # Final composite score (70% performance, 30% speed)\n",
    "        if speed_count > 0:\n",
    "            speed_score = speed_score / speed_count\n",
    "            composite_score = 0.7 * perf_score + 0.3 * speed_score\n",
    "        else:\n",
    "            composite_score = perf_score\n",
    "        \n",
    "        model_scores.append({\n",
    "            'model': model,\n",
    "            'performance_score': perf_score,\n",
    "            'speed_score': speed_score if speed_count > 0 else 0,\n",
    "            'composite_score': composite_score\n",
    "        })\n",
    "    \n",
    "    # Create ranking DataFrame\n",
    "    ranking_df = pd.DataFrame(model_scores)\n",
    "    ranking_df = ranking_df.sort_values('composite_score', ascending=False)\n",
    "    \n",
    "    print(\"\\nComposite Model Rankings (Performance + Speed):\")\n",
    "    print(ranking_df[['model', 'performance_score', 'speed_score', 'composite_score']].round(4).to_string(index=False))\n",
    "    \n",
    "    # Best model\n",
    "    best_model = ranking_df.iloc[0]\n",
    "    print(f\"\\nBest Overall Model: {best_model['model']} (Score: {best_model['composite_score']:.4f})\")\n",
    "else:\n",
    "    print(\"Insufficient data for ranking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Insights and Recommendations\n",
    "\n",
    "Key findings and actionable insights from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate insights based on analysis\n",
    "if 'model_name' in results_df.columns and metric_cols:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"KEY INSIGHTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Performance vs Speed trade-off\n",
    "    if 'metric_accuracy' in results_df.columns and 'latency_mean_ms' in results_df.columns:\n",
    "        accuracy_latency_corr = results_df['metric_accuracy'].corr(results_df['latency_mean_ms'])\n",
    "        print(f\"Accuracy vs Latency Correlation: {accuracy_latency_corr:.3f}\")\n",
    "        if accuracy_latency_corr < -0.5:\n",
    "            print(\"→ Strong negative correlation: Higher accuracy models tend to be slower\")\n",
    "        elif accuracy_latency_corr > 0.5:\n",
    "            print(\"→ Strong positive correlation: Higher accuracy models tend to be faster\")\n",
    "        else:\n",
    "            print(\"→ Weak correlation: Accuracy and speed are largely independent\")\n",
    "    \n",
    "    # Most consistent model\n",
    "    model_consistency = {}\n",
    "    for model in results_df['model_name'].unique():\n",
    "        model_data = results_df[results_df['model_name'] == model]\n",
    "        consistency_scores = []\n",
    "        \n",
    "        for metric in metric_cols[:3]:  # First 3 metrics\n",
    "            if metric in model_data.columns:\n",
    "                values = model_data[metric].dropna()\n",
    "                if len(values) > 1:\n",
    "                    # Lower coefficient of variation = more consistent\n",
    "                    cv = values.std() / values.mean()\n",
    "                    consistency_scores.append(cv)\n",
    "        \n",
    "        if consistency_scores:\n",
    "            model_consistency[model] = np.mean(consistency_scores)\n",
    "    \n",
    "    if model_consistency:\n",
    "        most_consistent = min(model_consistency, key=model_consistency.get)\n",
    "        print(f\"\\nMost Consistent Model: {most_consistent} (lowest performance variance)\")\n",
    "    \n",
    "    # Dataset performance patterns\n",
    "    if 'dataset' in results_df.columns:\n",
    "        dataset_performance = results_df.groupby('dataset')[metric_cols[0]].mean().sort_values(ascending=False)\n",
    "        easiest_dataset = dataset_performance.index[0]\n",
    "        hardest_dataset = dataset_performance.index[-1]\n",
    "        print(f\"\\nEasiest Dataset: {easiest_dataset} (avg {metric_cols[0].replace('metric_', '').title()}: {dataset_performance[easiest_dataset]:.4f})\")\n",
    "        print(f\"Hardest Dataset: {hardest_dataset} (avg {metric_cols[0].replace('metric_', '').title()}: {dataset_performance[hardest_dataset]:.4f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RECOMMENDATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Model selection recommendations\n",
    "    print(\"1. For production use:\")\n",
    "    print(\"   - Choose models with high composite scores\")\n",
    "    print(\"   - Consider latency requirements for your use case\")\n",
    "    print(\"   - Balance accuracy with computational resources\")\n",
    "    \n",
    "    print(\"2. For research:\")\n",
    "    print(\"   - Use ensemble of top-performing models\")\n",
    "    print(\"   - Analyze model behavior across different datasets\")\n",
    "    print(\"   - Consider statistical significance in claims\")\n",
    "    \n",
    "    print(\"3. For resource-constrained environments:\")\n",
    "    print(\"   - Prioritize smaller models (DistilBERT, TinyLlama)\")\n",
    "    print(\"   - Consider latency-optimized configurations\")\n",
    "    print(\"   - Use model quantization if available\")\n",
    "    \n",
    "    print(\"4. Future improvements:\")\n",
    "    print(\"   - Collect more diverse training data\")\n",
    "    print(\"   - Implement cross-dataset validation\")\n",
    "    print(\"   - Explore model ensembling techniques\")\n",
    "    print(\"   - Consider hardware-specific optimizations\")\n",
    "else:\n",
    "    print(\"Insufficient data for insights generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results\n",
    "\n",
    "Save your analysis results for further use or reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analysis results\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"analysis_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save summary statistics\n",
    "if 'model_name' in results_df.columns and metric_cols:\n",
    "    summary_stats = results_df.groupby('model_name')[metric_cols].agg(['mean', 'std', 'min', 'max', 'count'])\n",
    "    summary_stats.to_csv(f\"{output_dir}/model_summary.csv\")\n",
    "    \n",
    "    # Save rankings\n",
    "    if len(metric_cols) > 0:\n",
    "        accuracy_ranking = results_df.groupby('model_name')[metric_cols[0]].mean().sort_values(ascending=False)\n",
    "        accuracy_ranking.to_csv(f\"{output_dir}/accuracy_rankings.csv\")\n",
    "    \n",
    "    # Save insights\n",
    "    insights_file = f\"{output_dir}/insights_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "    with open(insights_file, 'w') as f:\n",
    "        f.write(\"# Model Comparison Analysis\\n\")\n",
    "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        f.write(f\"Models analyzed: {results_df['model_name'].nunique()}\\n\\n\")\n",
    "        f.write(f\"Datasets: {results_df['dataset'].nunique()}\\n\\n\")\n",
    "    \n",
    "    print(f\"\\nResults saved to {output_dir}/\")\n",
    "else:\n",
    "    print(\"No data available to export\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
}