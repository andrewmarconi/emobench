# Training configuration for SentiCompare benchmark

# Output directories
output:
  base_dir: "./experiments"
  checkpoints_dir: "./experiments/checkpoints"
  logs_dir: "./experiments/logs"
  results_dir: "./experiments/results"

# Training hyperparameters
training:
  num_epochs: 3
  learning_rate: 2e-4
  weight_decay: 0.01
  warmup_steps: 500
  max_grad_norm: 1.0  # Gradient clipping

  # Batch sizes per device (will be auto-adjusted based on device)
  batch_size:
    cuda: 8
    mps: 4
    cpu: 2

  eval_batch_size:
    cuda: 16
    mps: 8
    cpu: 4

  gradient_accumulation_steps: 4

  # Mixed precision training (device-specific)
  mixed_precision:
    cuda:
      fp16: true
      bf16: false
    mps:
      fp16: false  # Disabled for stability
      bf16: false
    cpu:
      fp16: false
      bf16: false

# Optimizer settings
optimizer:
  name: "adamw"
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8

# Learning rate scheduler
scheduler:
  type: "linear"  # Options: linear, cosine, constant
  num_warmup_steps: 500

# Evaluation strategy
evaluation:
  strategy: "steps"
  eval_steps: 500
  save_strategy: "steps"
  save_steps: 1000
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "f1"
  greater_is_better: true

# Logging
logging:
  strategy: "steps"
  steps: 100
  report_to: ["mlflow", "tensorboard"]
  log_level: "info"

# Early stopping
early_stopping:
  enabled: true
  patience: 3
  threshold: 0.001

# Checkpointing
checkpointing:
  save_safetensors: true
  save_optimizer: true
  save_scheduler: true

# Reproducibility
seed: 42
deterministic: false  # Set to true for full reproducibility (slower)

# Device settings
device:
  auto_detect: true
  # force_device: null  # Uncomment to force: "cuda", "mps", or "cpu"
  gpu_id: 0  # For multi-GPU setups

# Memory optimization
memory:
  gradient_checkpointing: false  # Enable for large models
  use_cpu_offload: false  # Offload to CPU when GPU memory is tight
  max_memory_per_gpu: null  # Auto-detect if null
