{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis Notebook\n",
    "\n",
    "This notebook provides comprehensive analysis of benchmark results from SentiCompare framework.\n",
    "## Usage\n",
    "\n",
    "1. Load your benchmark results\n",
    "2. Explore performance patterns and trends\n",
    "3. Analyze model behavior across datasets\n",
    "4. Generate insights and recommendations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuration\n",
    "RESULTS_PATH = \"experiments/results\"  # Adjust this path\n",
    "OUTPUT_DIR = \"analysis_outputs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Benchmark Results\n",
    "\n",
    "Load the benchmark results from your experiments directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load aggregated results\n",
    "try:\n",
    "    results_df = pd.read_csv(f\"{RESULTS_PATH}/aggregated_results.csv\")\n",
    "    print(f\"Loaded {len(results_df)} results from aggregated CSV\")\n",
    "except FileNotFoundError:\n",
    "    # Fallback: look for individual result files\n",
    "    import json\n",
    "    import glob\n",
    "    \n",
    "    result_files = glob.glob(f\"{RESULTS_PATH}/*.json\")\n",
    "    all_results = []\n",
    "    \n",
    "    for file_path in result_files:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            if isinstance(data, dict):\n",
    "                all_results.append(data)\n",
    "            elif isinstance(data, list):\n",
    "                all_results.extend(data)\n",
    "    \n",
    "    if all_results:\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        print(f\"Loaded {len(results_df)} results from {len(result_files)} JSON files\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No results found in {RESULTS_PATH}\")\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\nDataset shape: {results_df.shape}\")\n",
    "print(f\"\\nColumns: {list(results_df.columns)}\")\n",
    "print(f\"\\nModels: {results_df['model_name'].unique() if 'model_name' in results_df.columns else 'Unknown'}\")\n",
    "print(f\"\\nDatasets: {results_df['dataset'].unique() if 'dataset' in results_df.columns else 'Unknown'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performance Patterns Analysis\n",
    "\n",
    "Analyze performance patterns across models and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance distribution analysis\n",
    "metric_cols = [col for col in results_df.columns if col.startswith('metric_')]\n",
    "print(f\"\\nFound metrics: {[col.replace('metric_', '') for col in metric_cols]}\")\n",
    "\n",
    "# Create performance summary by model\n",
    "if 'model_name' in results_df.columns and metric_cols:\n",
    "    model_summary = results_df.groupby('model_name')[metric_cols].agg(['mean', 'std', 'min', 'max', 'count'])\n",
    "    \n",
    "    # Round for better display\n",
    "    model_summary = model_summary.round(4)\n",
    "    \n",
    "    print(\"\\nModel Performance Summary:\")\n",
    "    print(model_summary.to_string())\n",
    "    \n",
    "    # Performance distribution plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Model Performance Distribution', fontsize=16)\n",
    "    \n",
    "    for i, metric in enumerate(metric_cols[:4]):  # First 4 metrics\n",
    "        ax = axes[i//2, i%2]\n",
    "        \n",
    "        # Box plot for each model\n",
    "        model_data = []\n",
    "        model_labels = []\n",
    "        \n",
    "        for model in results_df['model_name'].unique():\n",
    "            model_values = results_df[results_df['model_name'] == model][metric].dropna()\n",
    "            if len(model_values) > 0:\n",
    "                model_data.append(model_values)\n",
    "                model_labels.append(model)\n",
    "        \n",
    "        if model_data:\n",
    "            ax.boxplot(model_data, labels=model_labels)\n",
    "            ax.set_title(f'{metric.replace(\"metric_\", \"\").title()}')\n",
    "            ax.set_ylabel(metric.replace(\"metric_\", \"\").title())\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No data available', ha='center', va='center', transform=ax.transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No model_name or metric columns found in results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset difficulty analysis\n",
    "if 'dataset' in results_df.columns and metric_cols:\n",
    "    # Analyze performance by dataset\n",
    "    dataset_performance = results_df.groupby('dataset')[metric_cols].mean()\n",
    "    \n",
    "    # Calculate dataset difficulty (inverse of average performance)\n",
    "    if 'metric_accuracy' in dataset_performance.columns:\n",
    "        dataset_difficulty = (1 - dataset_performance['metric_accuracy']).sort_values()\n",
    "        \n",
    "        print(\"\\nDataset Difficulty (based on accuracy):\")\n",
    "        print(dataset_difficulty)\n",
    "        \n",
    "        # Plot dataset difficulty\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        dataset_difficulty.plot(kind='bar')\n",
    "        plt.title('Dataset Difficulty (Lower Accuracy = Harder)')\n",
    "        plt.xlabel('Dataset')\n",
    "        plt.ylabel('Difficulty Score')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No dataset or accuracy data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis between metrics\n",
    "if len(metric_cols) >= 2:\n",
    "    # Calculate correlation matrix\n",
    "    metric_data = results_df[metric_cols].dropna()\n",
    "    correlation_matrix = metric_data.corr()\n",
    "    \n",
    "    print(\"\\nMetric Correlation Matrix:\")\n",
    "    print(correlation_matrix.round(3))\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                xticklabels=[col.replace('metric_', '') for col in metric_cols],\n",
    "                yticklabels=[col.replace('metric_', '') for col in metric_cols])\n",
    "    plt.title('Metric Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Need at least 2 metrics for correlation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Behavior Analysis\n",
    "\n",
    "Analyze how models perform across different datasets and conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model consistency analysis\n",
    "if 'model_name' in results_df.columns and 'dataset' in results_df.columns and metric_cols:\n",
    "    # Calculate performance variance for each model across datasets\n",
    "    model_consistency = {}\n",
    "    \n",
    "    for model in results_df['model_name'].unique():\n",
    "        model_data = results_df[results_df['model_name'] == model]\n",
    "        \n",
    "        # Calculate consistency scores for each metric\n",
    "        consistency_scores = []\n",
    "        \n",
    "        for metric in metric_cols[:3]:  # First 3 metrics\n",
    "            if metric in model_data.columns:\n",
    "                values = model_data[metric].dropna()\n",
    "                if len(values) > 1:\n",
    "                    # Lower coefficient of variation = more consistent\n",
    "                    cv = values.std() / values.mean()\n",
    "                    consistency_scores.append(cv)\n",
    "        \n",
    "        if consistency_scores:\n",
    "            model_consistency[model] = np.mean(consistency_scores)\n",
    "    \n",
    "    # Find most consistent model\n",
    "    if model_consistency:\n",
    "        most_consistent = min(model_consistency, key=model_consistency.get)\n",
    "        print(f\"\\nMost Consistent Model: {most_consistent} (lowest variance)\")\n",
    "        \n",
    "        # Plot consistency comparison\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        models = list(model_consistency.keys())\n",
    "        consistencies = list(model_consistency.values())\n",
    "        \n",
    "        plt.bar(models, consistencies)\n",
    "        plt.title('Model Consistency (Lower CV = More Consistent)')\n",
    "        plt.xlabel('Model')\n",
    "        plt.ylabel('Average Coefficient of Variation')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Insufficient data for consistency analysis\")\n",
    "else:\n",
    "    print(\"Required columns: model_name, dataset, and metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance vs Speed trade-off analysis\n",
    "if 'model_name' in results_df.columns and metric_cols:\n",
    "    # Find speed/latency metrics\n",
    "    speed_metrics = [col for col in results_df.columns if any(x in col.lower() for x in ['latency', 'throughput', 'time'])]\n",
    "    \n",
    "    if speed_metrics and 'metric_accuracy' in results_df.columns:\n",
    "        # Calculate efficiency scores (performance per speed unit)\n",
    "        model_efficiency = {}\n",
    "        \n",
    "        for model in results_df['model_name'].unique():\n",
    "            model_data = results_df[results_df['model_name'] == model]\n",
    "            \n",
    "            # Use first speed metric\n",
    "            speed_metric = speed_metrics[0]\n",
    "            \n",
    "            if speed_metric in model_data.columns and 'metric_accuracy' in model_data.columns:\n",
    "                avg_speed = model_data[speed_metric].mean()\n",
    "                avg_accuracy = model_data['metric_accuracy'].mean()\n",
    "                \n",
    "                # Efficiency = accuracy / speed (higher is better)\n",
    "                if 'latency' in speed_metric.lower():\n",
    "                    efficiency = avg_accuracy / (avg_speed / 1000)  # Convert ms to seconds\n",
    "                else:\n",
    "                    efficiency = avg_accuracy * avg_speed  # For throughput\n",
    "                \n",
    "                model_efficiency[model] = efficiency\n",
    "        \n",
    "        # Plot efficiency comparison\n",
    "        if model_efficiency:\n",
    "            efficiency_df = pd.DataFrame(list(model_efficiency.items()), columns=['model', 'efficiency_score'])\n",
    "            efficiency_df = efficiency_df.sort_values('efficiency_score', ascending=False)\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.bar(efficiency_df['model'], efficiency_df['efficiency_score'])\n",
    "            plt.title('Model Efficiency (Performance/Speed)')\n",
    "            plt.xlabel('Model')\n",
    "            plt.ylabel('Efficiency Score')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"\\nModel Efficiency Rankings:\")\n",
    "            print(efficiency_df)\n",
    "    else:\n",
    "        print(\"No speed or accuracy data available for efficiency analysis\")\n",
    "else:\n",
    "    print(\"Required columns for efficiency analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Insights\n",
    "\n",
    "Generate actionable insights from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive insights\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize insights\n",
    "insights = []\n",
    "\n",
    "# Performance insights\n",
    "if 'model_name' in results_df.columns and metric_cols:\n",
    "    # Best performing model\n",
    "    if 'metric_accuracy' in results_df.columns:\n",
    "        best_model = results_df.loc[results_df['metric_accuracy'].idxmax(), 'model_name']\n",
    "        best_accuracy = results_df['metric_accuracy'].max()\n",
    "        insights.append(f\"Best performing model: {best_model} (accuracy: {best_accuracy:.4f})\")\n",
    "    \n",
    "    # Performance range\n",
    "    accuracy_range = results_df['metric_accuracy'].max() - results_df['metric_accuracy'].min()\n",
    "    insights.append(f\"Accuracy range across models: {accuracy_range:.4f} ({accuracy_range/results_df['metric_accuracy'].mean()*100:.1f}% of mean)\")\n",
    "    \n",
    "    # Dataset insights\n",
    "    if 'dataset' in results_df.columns:\n",
    "        dataset_count = results_df['dataset'].nunique()\n",
    "        insights.append(f\"Evaluated across {dataset_count} datasets\")\n",
    "        \n",
    "        # Easiest and hardest datasets\n",
    "        if 'metric_accuracy' in results_df.columns:\n",
    "            dataset_performance = results_df.groupby('dataset')['metric_accuracy'].mean()\n",
    "            easiest_dataset = dataset_performance.idxmax()\n",
    "            hardest_dataset = dataset_performance.idxmin()\n",
    "            insights.append(f\"Easiest dataset: {easiest_dataset} (avg accuracy: {dataset_performance[easiest_dataset]:.4f})\")\n",
    "            insights.append(f\"Hardest dataset: {hardest_dataset} (avg accuracy: {dataset_performance[hardest_dataset]:.4f})\")\n",
    "\n",
    "# Model insights\n",
    "    model_count = results_df['model_name'].nunique()\n",
    "    insights.append(f\"Compared {model_count} different models\")\n",
    "    \n",
    "    # Model size distribution\n",
    "    model_sizes = {'tiny': 0, 'small': 0, 'medium': 0, 'large': 0}\n",
    "    for model in results_df['model_name'].unique():\n",
    "        model_lower = model.lower()\n",
    "        if 'tiny' in model_lower:\n",
    "            model_sizes['tiny'] += 1\n",
    "        elif 'distilbert' in model_lower or 'roberta-base' in model_lower:\n",
    "            model_sizes['small'] += 1\n",
    "        elif '1.1b' in model_lower or '1.5b' in model_lower or '1.7b' in model_lower:\n",
    "            model_sizes['medium'] += 1\n",
    "        elif '2b' in model_lower or '3' in model_lower:\n",
    "            model_sizes['large'] += 1\n",
    "    \n",
    "    insights.append(f\"Model size distribution: Tiny: {model_sizes['tiny']}, Small: {model_sizes['small']}, Medium: {model_sizes['medium']}, Large: {model_sizes['large']}\")\n",
    "\n",
    "# Recommendations\n",
    "insights.extend([\n",
    "    \"\",\n",
    "    \"RECOMMENDATIONS:\",\n",
    "    \"\",\n",
    "    \"1. For production use:\",\n",
    "    \"   - Select models with high accuracy and low latency\",\n",
    "    \"   - Consider model size vs available resources\",\n",
    "    \"   - Use ensemble methods for critical applications\",\n",
    "    \"\",\n",
    "    \"2. For research:\",\n",
    "    \"   - Analyze model behavior across diverse datasets\",\n",
    "    \"   - Consider statistical significance in claims\",\n",
    "    \"   - Explore model interpretability vs performance trade-offs\",\n",
    "    \"\",\n",
    "    \"3. For resource-constrained environments:\",\n",
    "    \"   - Prioritize smaller models (DistilBERT, TinyLlama)\",\n",
    "    \"   - Consider model quantization\",\n",
    "    \"   - Optimize for specific hardware (CPU/GPU/TPU)\",\n",
    "    \"\",\n",
    "    \"4. Future improvements:\",\n",
    "    \"   - Collect more diverse training data\",\n",
    "    \"   - Implement cross-dataset validation\",\n",
    "    \"   - Explore model ensembling techniques\",\n",
    "    \"   - Consider hardware-specific optimizations\",\n",
    "    \"   - Investigate model interpretability methods\"\n",
    "])\n",
    "\n",
    "# Save insights\n",
    "insights_file = f\"{OUTPUT_DIR}/analysis_insights_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "with open(insights_file, 'w') as f:\n",
    "    f.write(\"# SentiCompare Analysis Insights\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    f.write(f\"Models analyzed: {results_df['model_name'].nunique()}\\n\\n\")\n",
    "    f.write(f\"Datasets evaluated: {results_df['dataset'].nunique()}\\n\\n\")\n",
    "    f.write(f\"Total benchmark runs: {len(results_df)}\\n\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    for insight in insights:\n",
    "        f.write(f\"- {insight}\\n\")\n",
    "    \n",
    "print(f\"\\nAnalysis complete! Insights saved to {insights_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export Results\n",
    "\n",
    "Save your analysis results for documentation and sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export analysis results\n",
    "if 'model_name' in results_df.columns and metric_cols:\n",
    "    # Export detailed results\n",
    "    export_data = results_df.copy()\n",
    "    \n",
    "    # Add analysis metadata\n",
    "    export_data['analysis_timestamp'] = datetime.now().isoformat()\n",
    "    export_data['total_models'] = results_df['model_name'].nunique()\n",
    "    export_data['total_datasets'] = results_df['dataset'].nunique()\n",
    "    export_data['metrics_analyzed'] = [col.replace('metric_', '') for col in metric_cols]\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_file = f\"{OUTPUT_DIR}/detailed_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    export_data.to_csv(csv_file, index=False)\n",
    "    print(f\"Detailed analysis exported to {csv_file}\")\n",
    "    \n",
    "    # Save summary statistics\n",
    "    if 'model_name' in results_df.columns and metric_cols:\n",
    "        summary_stats = results_df.groupby('model_name')[metric_cols].agg(['mean', 'std', 'min', 'max', 'count'])\n",
    "        summary_file = f\"{OUTPUT_DIR}/summary_statistics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        summary_stats.to_csv(summary_file)\n",
    "        print(f\"Summary statistics exported to {summary_file}\")\n",
    "else:\n",
    "    print(\"No data available for export\")\n",
    "\n",
    "print(f\"\\nAnalysis complete! Results saved to {OUTPUT_DIR}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
}